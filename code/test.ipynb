{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import requests\n",
    "import httplib2\n",
    "import os\n",
    "from os.path import exists\n",
    "import csv\n",
    "import copy\n",
    "\n",
    "def cc(x):\n",
    "    return 5*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "def CoordinaryConjunction(txt):\n",
    "    tokenized = word_tokenize(txt)\n",
    "    posTag = nltk.pos_tag(tokenized)\n",
    "    count = 0\n",
    "    for tag in posTag:\n",
    "        if(tag[1] == 'CC'):\n",
    "            count+=+1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#txt = 'She tried on the dress'\n",
    "txt = open(\"checking.txt\", \"r\").read()\n",
    "\n",
    "def count_Phrasel_Verb(txt):\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    doc = nlp(txt)\n",
    "    arr = [ [word.lemma_ ,child.lemma_] for word in doc if word.pos_ == 'VERB' for child in word.children  if child.pos_ == 'ADP']\n",
    "    # for word in doc:\n",
    "    #     if word.pos_ != 'VERB':\n",
    "    #         continue\n",
    "    #     for child in word.children:\n",
    "    #         if child.pos_ == 'ADP':\n",
    "    #             print(child)\n",
    "    #print(len(arr))\n",
    "    return len(arr)\n",
    "\n",
    "count_Phrasel_Verb('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words = [\" and \", \" or \", \" nor \", \" if \", \" because \", \" unless \"] # to do\n",
    "\n",
    "def get_statistics(text, acc):\n",
    "    \n",
    "    for word in words:\n",
    "        if not (word in acc.keys()):\n",
    "            acc[word] = 0\n",
    "        acc[word] = acc[word] + text.count(word)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('went', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('st.', 'VB'),\n",
       " ('Khaleefa', 'NNP')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "txt = 'I went to st. Khaleefa'\n",
    "tokenized = word_tokenize(txt)\n",
    "posTag = nltk.pos_tag(tokenized)\n",
    "posTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "#txt = 'I had played football, tennis and basketball'\n",
    "\n",
    "\n",
    "#displacy.render(doc, page=\"false\", jupyter=True)\n",
    "\n",
    "#if their is a CC \n",
    "#calculate the normal conj from the nlp\n",
    "def CoordinaryConjunction(txt):\n",
    "\n",
    "    tokenized = word_tokenize(txt)\n",
    "    posTag = nltk.pos_tag(tokenized)\n",
    "    count = 0\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    doc = nlp(txt)\n",
    "    for word in posTag:\n",
    "        if word[1] == 'CC':\n",
    "            for token in doc:\n",
    "                if token.dep_ == 'conj':\n",
    "                    count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "#calculate the number of abbreviations from spacy (not working)\n",
    "def count_abbreviation(txt):\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    doc = nlp(txt)\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if token.like_num:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "txt = 'do you know where is st mohammed bin zayed '\n",
    "\n",
    "#txt = 'I will stay in the university or i want to go home.'\n",
    "doc = nlp(txt)\n",
    "#displacy.render(doc, page=\"false\", jupyter=True)\n",
    "count_abbreviation(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scispacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\makm7\\OneDrive\\سطح المكتب\\karam work\\artical_gatherer\\code\\test.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/makm7/OneDrive/%D8%B3%D8%B7%D8%AD%20%D8%A7%D9%84%D9%85%D9%83%D8%AA%D8%A8/karam%20work/artical_gatherer/code/test.ipynb#ch0000000?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/makm7/OneDrive/%D8%B3%D8%B7%D8%AD%20%D8%A7%D9%84%D9%85%D9%83%D8%AA%D8%A8/karam%20work/artical_gatherer/code/test.ipynb#ch0000000?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscispacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mabbreviation\u001b[39;00m \u001b[39mimport\u001b[39;00m AbbreviationDetector\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/makm7/OneDrive/%D8%B3%D8%B7%D8%AD%20%D8%A7%D9%84%D9%85%D9%83%D8%AA%D8%A8/karam%20work/artical_gatherer/code/test.ipynb#ch0000000?line=3'>4</a>\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39men_core_web_sm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/makm7/OneDrive/%D8%B3%D8%B7%D8%AD%20%D8%A7%D9%84%D9%85%D9%83%D8%AA%D8%A8/karam%20work/artical_gatherer/code/test.ipynb#ch0000000?line=5'>6</a>\u001b[0m abbreviation_pipe \u001b[39m=\u001b[39m AbbreviationDetector(nlp)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scispacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "abbreviation_pipe = AbbreviationDetector(nlp)\n",
    "nlp.add_pipe(abbreviation_pipe)\n",
    "\n",
    "text = \"StackOverflow is a question and answer site for professional and enthusiast programmers. SO rocks!\"\n",
    "\n",
    "def replace_acronyms(text):\n",
    "    doc = nlp(text)\n",
    "    altered_tok = [tok.text for tok in doc]\n",
    "    for abrv in doc._.abbreviations:\n",
    "        altered_tok[abrv.start] = str(abrv._.long_form)\n",
    "\n",
    "    return(\" \".join(altered_tok))\n",
    "\n",
    "replace_acronyms(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(cookies, cream)]\n"
     ]
    }
   ],
   "source": [
    "arr = [ [word.lemma_ ,child.lemma_] for word in doc if word.pos_ == 'VERB' for child in word.children  if child.pos_ == 'ADP']\n",
    " \n",
    "# crtl k c -> comment\n",
    "# crtl k u -> uncomment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n"
     ]
    }
   ],
   "source": [
    "#txt = 'She tried on the dress'\n",
    "#txt = open(\"checking.txt\", \"r\").read()\n",
    "\n",
    "#nlp = spacy.load(\"en_core_web_lg\")\n",
    "#doc = nlp(txt)\n",
    "arr = [ [word.lemma_ ,child.lemma_] for word in doc if word.pos_ == 'VERB' for child in word.children  if child.pos_ == 'ADP']\n",
    "print(len(arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mainConfig import *\n",
    "import functions as Fun\n",
    "d = Fun.compine_categ(categories[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\makm7\\OneDrive\\سطح المكتب\\karam work\\artical_gatherer\\code\\test.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/makm7/OneDrive/%D8%B3%D8%B7%D8%AD%20%D8%A7%D9%84%D9%85%D9%83%D8%AA%D8%A8/karam%20work/artical_gatherer/code/test.ipynb#ch0000000?line=0'>1</a>\u001b[0m num_word \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(d\u001b[39m.\u001b[39;49msplit())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "num_word = len(d.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "token = sent_tokenize(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30954"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "res = [nlp(text)for text in token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arr = [ [word.lemma_ ,child.lemma_] for doc in res for word in doc if word.pos_ == 'VERB' for child in word.children  if child.pos_ == 'ADP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.563085282610634"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*len(arr)/num_word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f52a3f3578b02bbf604551531def6b1550332462d8e3ad268b50e01ccd36abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
