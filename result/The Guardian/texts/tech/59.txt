The ‘Lookalike Audience’ tool relies on algorithm that US said discriminates on the basis of race, sex and other characteristics
Facebook will change its algorithms to prevent discriminatory housing advertising and its parent company will subject itself to court oversight to settle a lawsuit brought by the US Department of Justice on Tuesday.
In a release, US government officials said that Meta, formerly known as Facebook, reached an agreement to settle the lawsuit filed the same day in Manhattan federal court.
According to terms of the settlement, Facebook will stop using an advertising tool for housing ads that the government said employed a discriminatory algorithm to locate users who “look like” other users based on characteristics protected by the Fair Housing Act, the Justice Department said. By 31 December, Facebook must stop using the tool once called “Lookalike Audience”, which relies on an algorithm that the US said discriminates on the basis of race, sex and other characteristics.
Facebook also will develop a new system over the next half-year to address racial and other disparities caused by its use of personalization algorithms in its delivery system for housing ads, it said.
According to the release, it was the justice department’s first case challenging algorithmic discrimination under the Fair Housing Act. Facebook will now be subject to justice department approval and court oversight for its ad targeting and delivery system.
US attorney Damian Williams called the lawsuit “groundbreaking.” Assistant attorney general Kristen Clarke called it “historic”.
Ashley Settle, a Facebook spokesperson, said in an email that the company was “building a novel machine learning method without our ads system that will change the way housing ads are delivered to people residing in the US across different demographic groups.”
She said the company would extend its new method for ads related to employment and credit in the US. “We are excited to pioneer this effort,” Settle added in an email.
Williams said Facebook’s technology has in the past violated the Fair Housing Act online “just as when companies engage in discriminatory advertising using more traditional advertising methods”.
Clarke said “companies like Meta have a responsibility to ensure their algorithmic tools are not used in a discriminatory manner.”
The announcement comes after Facebook already agreed in March 2019 to overhaul its ad-targeting systems to prevent discrimination in housing, credit and employment ads as part of a legal settlement with a group including the American Civil Liberties Union, the National Fair Housing Alliance and others.
The changes announced then were designed so that advertisers who wanted to run housing, employment or credit ads would no longer be allowed to target people by age, gender or zip code.
The justice department said Tuesday that the 2019 settlement reduced the potentially discriminatory targeting options available to advertisers but failed to resolve other problems, including Facebook’s discriminatory delivery of housing ads through machine-learning algorithms.
